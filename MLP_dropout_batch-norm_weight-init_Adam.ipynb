{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## MLP 기본 만들기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 01\n",
    "1) Torch.cuda.is_available() : 가능한 gpu가 있는지 확인해서 train에 이용할 device 결정\\\n",
    "2) BATCH_SIZE, EPOCHS 등 하이퍼파라미터 결정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version  :  1.9.0+cu111  Device :  cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version  : ', torch.__version__, ' Device : ', DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 02\n",
    "1) dataset 과 dataloader 준비\n",
    "2) 데이터 확인하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kist/anaconda3/envs/sj/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train :  torch.Size([32, 1, 28, 28])  type :  torch.FloatTensor\n",
      "y_train :  torch.Size([32])  type :  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader :\n",
    "    print('X_train : ', X_train.size(), ' type : ', X_train.type())\n",
    "    print('y_train : ', y_train.size(), ' type : ', y_train.type())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x72 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1\n",
    "plt.figure(figsize = (10 * pltsize, pltsize))\n",
    "for i in range(10) :\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = 'gray_r')\n",
    "    plt.title('Class : ' + str(y_train[i].item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 03\n",
    "1) 모델 구조 정의하기\n",
    "2) optimizer, loss function 정의하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5 # 50%의 노드에 대해 weights 계산하지 않음\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x) # batch normalization : activation function 앞 뒤에 batch_norm을 할 지는 연구자마다 갈림\n",
    "        x = F.relu(x) # activation function : ReLU\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob) # self.training = True 일 때 만, 즉 학습 할 때만 dropout 적용하기\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "#He Initialization 을 이용해서 weight 초기화 하기\n",
    "def weight_init(m) : # model 내에서 weight 초기화할 부분을 정하기 위한 함수\n",
    "    if isinstance(m, nn.Linear) : # 모델 내 파라미터들 중에서 nn.Linear 의 weight만 지정\n",
    "        init.kaiming_uniform_(m.weight.data) # nn.Linear 파라미터에 대해서 He Initialization 진행\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01) # optimizer 를 Adam 으로 진행\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 04\n",
    "- train 코드"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train() # model의 상위 클래스 속 변수 self.training = True 로 만들어주기\n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = model(image)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad() # 이전에 학습할 때 저장된 gradient 없애기\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch : {} [ {}/{} ({:.0f}%) ]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 05\n",
    "- evaluate 코드"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval() # model의 상위 클래스 속 변수 self.training = False 로 만들어주기\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad() : #train 단계가 아니라 eval 단계이므로 gradient update 못하게 방지\n",
    "        for image, label in test_loader :\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#STEP 06\n",
    "- Train 및 Test 하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [ 0/60000 (0%) ]\tTrain Loss : 2.984270\n",
      "Train Epoch : 1 [ 6400/60000 (11%) ]\tTrain Loss : 0.341239\n",
      "Train Epoch : 1 [ 12800/60000 (21%) ]\tTrain Loss : 0.263182\n",
      "Train Epoch : 1 [ 19200/60000 (32%) ]\tTrain Loss : 0.365248\n",
      "Train Epoch : 1 [ 25600/60000 (43%) ]\tTrain Loss : 0.548437\n",
      "Train Epoch : 1 [ 32000/60000 (53%) ]\tTrain Loss : 0.095029\n",
      "Train Epoch : 1 [ 38400/60000 (64%) ]\tTrain Loss : 0.275530\n",
      "Train Epoch : 1 [ 44800/60000 (75%) ]\tTrain Loss : 0.377446\n",
      "Train Epoch : 1 [ 51200/60000 (85%) ]\tTrain Loss : 0.429867\n",
      "Train Epoch : 1 [ 57600/60000 (96%) ]\tTrain Loss : 0.154104\n",
      "\n",
      "[Epochh : 1], \tTest Loss : 0.0039, \tTest Accurary : 96.12 % \n",
      "\n",
      "Train Epoch : 2 [ 0/60000 (0%) ]\tTrain Loss : 0.245688\n",
      "Train Epoch : 2 [ 6400/60000 (11%) ]\tTrain Loss : 0.159858\n",
      "Train Epoch : 2 [ 12800/60000 (21%) ]\tTrain Loss : 0.113497\n",
      "Train Epoch : 2 [ 19200/60000 (32%) ]\tTrain Loss : 0.183045\n",
      "Train Epoch : 2 [ 25600/60000 (43%) ]\tTrain Loss : 0.558814\n",
      "Train Epoch : 2 [ 32000/60000 (53%) ]\tTrain Loss : 0.190183\n",
      "Train Epoch : 2 [ 38400/60000 (64%) ]\tTrain Loss : 0.537844\n",
      "Train Epoch : 2 [ 44800/60000 (75%) ]\tTrain Loss : 0.267822\n",
      "Train Epoch : 2 [ 51200/60000 (85%) ]\tTrain Loss : 0.324034\n",
      "Train Epoch : 2 [ 57600/60000 (96%) ]\tTrain Loss : 0.055227\n",
      "\n",
      "[Epochh : 2], \tTest Loss : 0.0032, \tTest Accurary : 96.82 % \n",
      "\n",
      "Train Epoch : 3 [ 0/60000 (0%) ]\tTrain Loss : 0.249200\n",
      "Train Epoch : 3 [ 6400/60000 (11%) ]\tTrain Loss : 0.669901\n",
      "Train Epoch : 3 [ 12800/60000 (21%) ]\tTrain Loss : 0.112518\n",
      "Train Epoch : 3 [ 19200/60000 (32%) ]\tTrain Loss : 0.104389\n",
      "Train Epoch : 3 [ 25600/60000 (43%) ]\tTrain Loss : 0.015530\n",
      "Train Epoch : 3 [ 32000/60000 (53%) ]\tTrain Loss : 0.161220\n",
      "Train Epoch : 3 [ 38400/60000 (64%) ]\tTrain Loss : 0.087450\n",
      "Train Epoch : 3 [ 44800/60000 (75%) ]\tTrain Loss : 0.129679\n",
      "Train Epoch : 3 [ 51200/60000 (85%) ]\tTrain Loss : 0.282472\n",
      "Train Epoch : 3 [ 57600/60000 (96%) ]\tTrain Loss : 0.194088\n",
      "\n",
      "[Epochh : 3], \tTest Loss : 0.0029, \tTest Accurary : 97.04 % \n",
      "\n",
      "Train Epoch : 4 [ 0/60000 (0%) ]\tTrain Loss : 0.259756\n",
      "Train Epoch : 4 [ 6400/60000 (11%) ]\tTrain Loss : 0.523391\n",
      "Train Epoch : 4 [ 12800/60000 (21%) ]\tTrain Loss : 0.031140\n",
      "Train Epoch : 4 [ 19200/60000 (32%) ]\tTrain Loss : 0.084341\n",
      "Train Epoch : 4 [ 25600/60000 (43%) ]\tTrain Loss : 0.377768\n",
      "Train Epoch : 4 [ 32000/60000 (53%) ]\tTrain Loss : 0.090560\n",
      "Train Epoch : 4 [ 38400/60000 (64%) ]\tTrain Loss : 0.124048\n",
      "Train Epoch : 4 [ 44800/60000 (75%) ]\tTrain Loss : 0.322841\n",
      "Train Epoch : 4 [ 51200/60000 (85%) ]\tTrain Loss : 0.400765\n",
      "Train Epoch : 4 [ 57600/60000 (96%) ]\tTrain Loss : 0.218509\n",
      "\n",
      "[Epochh : 4], \tTest Loss : 0.0026, \tTest Accurary : 97.48 % \n",
      "\n",
      "Train Epoch : 5 [ 0/60000 (0%) ]\tTrain Loss : 0.293042\n",
      "Train Epoch : 5 [ 6400/60000 (11%) ]\tTrain Loss : 0.285479\n",
      "Train Epoch : 5 [ 12800/60000 (21%) ]\tTrain Loss : 0.358764\n",
      "Train Epoch : 5 [ 19200/60000 (32%) ]\tTrain Loss : 0.039830\n",
      "Train Epoch : 5 [ 25600/60000 (43%) ]\tTrain Loss : 0.162209\n",
      "Train Epoch : 5 [ 32000/60000 (53%) ]\tTrain Loss : 0.145172\n",
      "Train Epoch : 5 [ 38400/60000 (64%) ]\tTrain Loss : 0.183884\n",
      "Train Epoch : 5 [ 44800/60000 (75%) ]\tTrain Loss : 0.282162\n",
      "Train Epoch : 5 [ 51200/60000 (85%) ]\tTrain Loss : 0.520084\n",
      "Train Epoch : 5 [ 57600/60000 (96%) ]\tTrain Loss : 0.225391\n",
      "\n",
      "[Epochh : 5], \tTest Loss : 0.0026, \tTest Accurary : 97.52 % \n",
      "\n",
      "Train Epoch : 6 [ 0/60000 (0%) ]\tTrain Loss : 0.352827\n",
      "Train Epoch : 6 [ 6400/60000 (11%) ]\tTrain Loss : 0.071805\n",
      "Train Epoch : 6 [ 12800/60000 (21%) ]\tTrain Loss : 0.345806\n",
      "Train Epoch : 6 [ 19200/60000 (32%) ]\tTrain Loss : 0.167585\n",
      "Train Epoch : 6 [ 25600/60000 (43%) ]\tTrain Loss : 0.276617\n",
      "Train Epoch : 6 [ 32000/60000 (53%) ]\tTrain Loss : 0.240895\n",
      "Train Epoch : 6 [ 38400/60000 (64%) ]\tTrain Loss : 0.229279\n",
      "Train Epoch : 6 [ 44800/60000 (75%) ]\tTrain Loss : 0.048376\n",
      "Train Epoch : 6 [ 51200/60000 (85%) ]\tTrain Loss : 0.322354\n",
      "Train Epoch : 6 [ 57600/60000 (96%) ]\tTrain Loss : 0.153980\n",
      "\n",
      "[Epochh : 6], \tTest Loss : 0.0025, \tTest Accurary : 97.52 % \n",
      "\n",
      "Train Epoch : 7 [ 0/60000 (0%) ]\tTrain Loss : 0.472178\n",
      "Train Epoch : 7 [ 6400/60000 (11%) ]\tTrain Loss : 0.175555\n",
      "Train Epoch : 7 [ 12800/60000 (21%) ]\tTrain Loss : 0.070765\n",
      "Train Epoch : 7 [ 19200/60000 (32%) ]\tTrain Loss : 0.114685\n",
      "Train Epoch : 7 [ 25600/60000 (43%) ]\tTrain Loss : 0.042280\n",
      "Train Epoch : 7 [ 32000/60000 (53%) ]\tTrain Loss : 0.072389\n",
      "Train Epoch : 7 [ 38400/60000 (64%) ]\tTrain Loss : 0.023871\n",
      "Train Epoch : 7 [ 44800/60000 (75%) ]\tTrain Loss : 0.088939\n",
      "Train Epoch : 7 [ 51200/60000 (85%) ]\tTrain Loss : 0.139742\n",
      "Train Epoch : 7 [ 57600/60000 (96%) ]\tTrain Loss : 0.084197\n",
      "\n",
      "[Epochh : 7], \tTest Loss : 0.0025, \tTest Accurary : 97.55 % \n",
      "\n",
      "Train Epoch : 8 [ 0/60000 (0%) ]\tTrain Loss : 0.112461\n",
      "Train Epoch : 8 [ 6400/60000 (11%) ]\tTrain Loss : 0.246307\n",
      "Train Epoch : 8 [ 12800/60000 (21%) ]\tTrain Loss : 0.398316\n",
      "Train Epoch : 8 [ 19200/60000 (32%) ]\tTrain Loss : 0.096215\n",
      "Train Epoch : 8 [ 25600/60000 (43%) ]\tTrain Loss : 0.064616\n",
      "Train Epoch : 8 [ 32000/60000 (53%) ]\tTrain Loss : 0.018060\n",
      "Train Epoch : 8 [ 38400/60000 (64%) ]\tTrain Loss : 0.072520\n",
      "Train Epoch : 8 [ 44800/60000 (75%) ]\tTrain Loss : 0.050288\n",
      "Train Epoch : 8 [ 51200/60000 (85%) ]\tTrain Loss : 0.151657\n",
      "Train Epoch : 8 [ 57600/60000 (96%) ]\tTrain Loss : 0.041869\n",
      "\n",
      "[Epochh : 8], \tTest Loss : 0.0023, \tTest Accurary : 97.61 % \n",
      "\n",
      "Train Epoch : 9 [ 0/60000 (0%) ]\tTrain Loss : 0.250821\n",
      "Train Epoch : 9 [ 6400/60000 (11%) ]\tTrain Loss : 0.012461\n",
      "Train Epoch : 9 [ 12800/60000 (21%) ]\tTrain Loss : 0.028463\n",
      "Train Epoch : 9 [ 19200/60000 (32%) ]\tTrain Loss : 0.287419\n",
      "Train Epoch : 9 [ 25600/60000 (43%) ]\tTrain Loss : 0.227285\n",
      "Train Epoch : 9 [ 32000/60000 (53%) ]\tTrain Loss : 0.191602\n",
      "Train Epoch : 9 [ 38400/60000 (64%) ]\tTrain Loss : 0.032786\n",
      "Train Epoch : 9 [ 44800/60000 (75%) ]\tTrain Loss : 0.027112\n",
      "Train Epoch : 9 [ 51200/60000 (85%) ]\tTrain Loss : 0.274064\n",
      "Train Epoch : 9 [ 57600/60000 (96%) ]\tTrain Loss : 0.376269\n",
      "\n",
      "[Epochh : 9], \tTest Loss : 0.0023, \tTest Accurary : 97.69 % \n",
      "\n",
      "Train Epoch : 10 [ 0/60000 (0%) ]\tTrain Loss : 0.066323\n",
      "Train Epoch : 10 [ 6400/60000 (11%) ]\tTrain Loss : 0.096797\n",
      "Train Epoch : 10 [ 12800/60000 (21%) ]\tTrain Loss : 0.047651\n",
      "Train Epoch : 10 [ 19200/60000 (32%) ]\tTrain Loss : 0.169354\n",
      "Train Epoch : 10 [ 25600/60000 (43%) ]\tTrain Loss : 0.021408\n",
      "Train Epoch : 10 [ 32000/60000 (53%) ]\tTrain Loss : 0.497260\n",
      "Train Epoch : 10 [ 38400/60000 (64%) ]\tTrain Loss : 0.075962\n",
      "Train Epoch : 10 [ 44800/60000 (75%) ]\tTrain Loss : 0.072617\n",
      "Train Epoch : 10 [ 51200/60000 (85%) ]\tTrain Loss : 0.192442\n",
      "Train Epoch : 10 [ 57600/60000 (96%) ]\tTrain Loss : 0.114929\n",
      "\n",
      "[Epochh : 10], \tTest Loss : 0.0022, \tTest Accurary : 97.90 % \n",
      "\n",
      "Train Epoch : 11 [ 0/60000 (0%) ]\tTrain Loss : 0.025216\n",
      "Train Epoch : 11 [ 6400/60000 (11%) ]\tTrain Loss : 0.047686\n",
      "Train Epoch : 11 [ 12800/60000 (21%) ]\tTrain Loss : 0.117540\n",
      "Train Epoch : 11 [ 19200/60000 (32%) ]\tTrain Loss : 0.276100\n",
      "Train Epoch : 11 [ 25600/60000 (43%) ]\tTrain Loss : 0.115832\n",
      "Train Epoch : 11 [ 32000/60000 (53%) ]\tTrain Loss : 0.234005\n",
      "Train Epoch : 11 [ 38400/60000 (64%) ]\tTrain Loss : 0.033277\n",
      "Train Epoch : 11 [ 44800/60000 (75%) ]\tTrain Loss : 0.093131\n",
      "Train Epoch : 11 [ 51200/60000 (85%) ]\tTrain Loss : 0.026772\n",
      "Train Epoch : 11 [ 57600/60000 (96%) ]\tTrain Loss : 0.040125\n",
      "\n",
      "[Epochh : 11], \tTest Loss : 0.0020, \tTest Accurary : 98.10 % \n",
      "\n",
      "Train Epoch : 12 [ 0/60000 (0%) ]\tTrain Loss : 0.150882\n",
      "Train Epoch : 12 [ 6400/60000 (11%) ]\tTrain Loss : 0.077984\n",
      "Train Epoch : 12 [ 12800/60000 (21%) ]\tTrain Loss : 0.295939\n",
      "Train Epoch : 12 [ 19200/60000 (32%) ]\tTrain Loss : 0.074400\n",
      "Train Epoch : 12 [ 25600/60000 (43%) ]\tTrain Loss : 0.046140\n",
      "Train Epoch : 12 [ 32000/60000 (53%) ]\tTrain Loss : 0.275297\n",
      "Train Epoch : 12 [ 38400/60000 (64%) ]\tTrain Loss : 0.151583\n",
      "Train Epoch : 12 [ 44800/60000 (75%) ]\tTrain Loss : 0.119055\n",
      "Train Epoch : 12 [ 51200/60000 (85%) ]\tTrain Loss : 0.130630\n",
      "Train Epoch : 12 [ 57600/60000 (96%) ]\tTrain Loss : 0.360957\n",
      "\n",
      "[Epochh : 12], \tTest Loss : 0.0021, \tTest Accurary : 97.93 % \n",
      "\n",
      "Train Epoch : 13 [ 0/60000 (0%) ]\tTrain Loss : 0.208422\n",
      "Train Epoch : 13 [ 6400/60000 (11%) ]\tTrain Loss : 0.139583\n",
      "Train Epoch : 13 [ 12800/60000 (21%) ]\tTrain Loss : 0.090077\n",
      "Train Epoch : 13 [ 19200/60000 (32%) ]\tTrain Loss : 0.050852\n",
      "Train Epoch : 13 [ 25600/60000 (43%) ]\tTrain Loss : 0.264081\n",
      "Train Epoch : 13 [ 32000/60000 (53%) ]\tTrain Loss : 0.042983\n",
      "Train Epoch : 13 [ 38400/60000 (64%) ]\tTrain Loss : 0.060519\n",
      "Train Epoch : 13 [ 44800/60000 (75%) ]\tTrain Loss : 0.043428\n",
      "Train Epoch : 13 [ 51200/60000 (85%) ]\tTrain Loss : 0.106968\n",
      "Train Epoch : 13 [ 57600/60000 (96%) ]\tTrain Loss : 0.058448\n",
      "\n",
      "[Epochh : 13], \tTest Loss : 0.0022, \tTest Accurary : 97.87 % \n",
      "\n",
      "Train Epoch : 14 [ 0/60000 (0%) ]\tTrain Loss : 0.205253\n",
      "Train Epoch : 14 [ 6400/60000 (11%) ]\tTrain Loss : 0.416447\n",
      "Train Epoch : 14 [ 12800/60000 (21%) ]\tTrain Loss : 0.107696\n",
      "Train Epoch : 14 [ 19200/60000 (32%) ]\tTrain Loss : 0.019014\n",
      "Train Epoch : 14 [ 25600/60000 (43%) ]\tTrain Loss : 0.140273\n",
      "Train Epoch : 14 [ 32000/60000 (53%) ]\tTrain Loss : 0.017077\n",
      "Train Epoch : 14 [ 38400/60000 (64%) ]\tTrain Loss : 0.285084\n",
      "Train Epoch : 14 [ 44800/60000 (75%) ]\tTrain Loss : 0.217674\n",
      "Train Epoch : 14 [ 51200/60000 (85%) ]\tTrain Loss : 0.167629\n",
      "Train Epoch : 14 [ 57600/60000 (96%) ]\tTrain Loss : 0.256149\n",
      "\n",
      "[Epochh : 14], \tTest Loss : 0.0020, \tTest Accurary : 98.14 % \n",
      "\n",
      "Train Epoch : 15 [ 0/60000 (0%) ]\tTrain Loss : 0.034613\n",
      "Train Epoch : 15 [ 6400/60000 (11%) ]\tTrain Loss : 0.273618\n",
      "Train Epoch : 15 [ 12800/60000 (21%) ]\tTrain Loss : 0.122178\n",
      "Train Epoch : 15 [ 19200/60000 (32%) ]\tTrain Loss : 0.378459\n",
      "Train Epoch : 15 [ 25600/60000 (43%) ]\tTrain Loss : 0.071582\n",
      "Train Epoch : 15 [ 32000/60000 (53%) ]\tTrain Loss : 0.070934\n",
      "Train Epoch : 15 [ 38400/60000 (64%) ]\tTrain Loss : 0.440532\n",
      "Train Epoch : 15 [ 44800/60000 (75%) ]\tTrain Loss : 0.034480\n",
      "Train Epoch : 15 [ 51200/60000 (85%) ]\tTrain Loss : 0.059707\n",
      "Train Epoch : 15 [ 57600/60000 (96%) ]\tTrain Loss : 0.222518\n",
      "\n",
      "[Epochh : 15], \tTest Loss : 0.0019, \tTest Accurary : 98.05 % \n",
      "\n",
      "Train Epoch : 16 [ 0/60000 (0%) ]\tTrain Loss : 0.115526\n",
      "Train Epoch : 16 [ 6400/60000 (11%) ]\tTrain Loss : 0.135943\n",
      "Train Epoch : 16 [ 12800/60000 (21%) ]\tTrain Loss : 0.017596\n",
      "Train Epoch : 16 [ 19200/60000 (32%) ]\tTrain Loss : 0.008694\n",
      "Train Epoch : 16 [ 25600/60000 (43%) ]\tTrain Loss : 0.305914\n",
      "Train Epoch : 16 [ 32000/60000 (53%) ]\tTrain Loss : 0.143060\n",
      "Train Epoch : 16 [ 38400/60000 (64%) ]\tTrain Loss : 0.059190\n",
      "Train Epoch : 16 [ 44800/60000 (75%) ]\tTrain Loss : 0.257075\n",
      "Train Epoch : 16 [ 51200/60000 (85%) ]\tTrain Loss : 0.334416\n",
      "Train Epoch : 16 [ 57600/60000 (96%) ]\tTrain Loss : 0.639948\n",
      "\n",
      "[Epochh : 16], \tTest Loss : 0.0020, \tTest Accurary : 98.13 % \n",
      "\n",
      "Train Epoch : 17 [ 0/60000 (0%) ]\tTrain Loss : 0.114621\n",
      "Train Epoch : 17 [ 6400/60000 (11%) ]\tTrain Loss : 0.226590\n",
      "Train Epoch : 17 [ 12800/60000 (21%) ]\tTrain Loss : 0.096414\n",
      "Train Epoch : 17 [ 19200/60000 (32%) ]\tTrain Loss : 0.124417\n",
      "Train Epoch : 17 [ 25600/60000 (43%) ]\tTrain Loss : 0.115603\n",
      "Train Epoch : 17 [ 32000/60000 (53%) ]\tTrain Loss : 0.017252\n",
      "Train Epoch : 17 [ 38400/60000 (64%) ]\tTrain Loss : 0.122666\n",
      "Train Epoch : 17 [ 44800/60000 (75%) ]\tTrain Loss : 0.011019\n",
      "Train Epoch : 17 [ 51200/60000 (85%) ]\tTrain Loss : 0.200672\n",
      "Train Epoch : 17 [ 57600/60000 (96%) ]\tTrain Loss : 0.220321\n",
      "\n",
      "[Epochh : 17], \tTest Loss : 0.0020, \tTest Accurary : 98.19 % \n",
      "\n",
      "Train Epoch : 18 [ 0/60000 (0%) ]\tTrain Loss : 0.278204\n",
      "Train Epoch : 18 [ 6400/60000 (11%) ]\tTrain Loss : 0.220102\n",
      "Train Epoch : 18 [ 12800/60000 (21%) ]\tTrain Loss : 0.123724\n",
      "Train Epoch : 18 [ 19200/60000 (32%) ]\tTrain Loss : 0.029793\n",
      "Train Epoch : 18 [ 25600/60000 (43%) ]\tTrain Loss : 0.033876\n",
      "Train Epoch : 18 [ 32000/60000 (53%) ]\tTrain Loss : 0.159169\n",
      "Train Epoch : 18 [ 38400/60000 (64%) ]\tTrain Loss : 0.030270\n",
      "Train Epoch : 18 [ 44800/60000 (75%) ]\tTrain Loss : 0.175025\n",
      "Train Epoch : 18 [ 51200/60000 (85%) ]\tTrain Loss : 0.121218\n",
      "Train Epoch : 18 [ 57600/60000 (96%) ]\tTrain Loss : 0.311145\n",
      "\n",
      "[Epochh : 18], \tTest Loss : 0.0021, \tTest Accurary : 98.11 % \n",
      "\n",
      "Train Epoch : 19 [ 0/60000 (0%) ]\tTrain Loss : 0.131154\n",
      "Train Epoch : 19 [ 6400/60000 (11%) ]\tTrain Loss : 0.061467\n",
      "Train Epoch : 19 [ 12800/60000 (21%) ]\tTrain Loss : 0.158285\n",
      "Train Epoch : 19 [ 19200/60000 (32%) ]\tTrain Loss : 0.059155\n",
      "Train Epoch : 19 [ 25600/60000 (43%) ]\tTrain Loss : 0.305339\n",
      "Train Epoch : 19 [ 32000/60000 (53%) ]\tTrain Loss : 0.106931\n",
      "Train Epoch : 19 [ 38400/60000 (64%) ]\tTrain Loss : 0.078063\n",
      "Train Epoch : 19 [ 44800/60000 (75%) ]\tTrain Loss : 0.006979\n",
      "Train Epoch : 19 [ 51200/60000 (85%) ]\tTrain Loss : 0.050341\n",
      "Train Epoch : 19 [ 57600/60000 (96%) ]\tTrain Loss : 0.111799\n",
      "\n",
      "[Epochh : 19], \tTest Loss : 0.0023, \tTest Accurary : 98.02 % \n",
      "\n",
      "Train Epoch : 20 [ 0/60000 (0%) ]\tTrain Loss : 0.017491\n",
      "Train Epoch : 20 [ 6400/60000 (11%) ]\tTrain Loss : 0.014968\n",
      "Train Epoch : 20 [ 12800/60000 (21%) ]\tTrain Loss : 0.427262\n",
      "Train Epoch : 20 [ 19200/60000 (32%) ]\tTrain Loss : 0.014408\n",
      "Train Epoch : 20 [ 25600/60000 (43%) ]\tTrain Loss : 0.017372\n",
      "Train Epoch : 20 [ 32000/60000 (53%) ]\tTrain Loss : 0.044163\n",
      "Train Epoch : 20 [ 38400/60000 (64%) ]\tTrain Loss : 0.026098\n",
      "Train Epoch : 20 [ 44800/60000 (75%) ]\tTrain Loss : 0.124766\n",
      "Train Epoch : 20 [ 51200/60000 (85%) ]\tTrain Loss : 0.116240\n",
      "Train Epoch : 20 [ 57600/60000 (96%) ]\tTrain Loss : 0.169512\n",
      "\n",
      "[Epochh : 20], \tTest Loss : 0.0021, \tTest Accurary : 98.01 % \n",
      "\n",
      "Train Epoch : 21 [ 0/60000 (0%) ]\tTrain Loss : 0.096679\n",
      "Train Epoch : 21 [ 6400/60000 (11%) ]\tTrain Loss : 0.045090\n",
      "Train Epoch : 21 [ 12800/60000 (21%) ]\tTrain Loss : 0.397660\n",
      "Train Epoch : 21 [ 19200/60000 (32%) ]\tTrain Loss : 0.022854\n",
      "Train Epoch : 21 [ 25600/60000 (43%) ]\tTrain Loss : 0.115549\n",
      "Train Epoch : 21 [ 32000/60000 (53%) ]\tTrain Loss : 0.010563\n",
      "Train Epoch : 21 [ 38400/60000 (64%) ]\tTrain Loss : 0.027153\n",
      "Train Epoch : 21 [ 44800/60000 (75%) ]\tTrain Loss : 0.025205\n",
      "Train Epoch : 21 [ 51200/60000 (85%) ]\tTrain Loss : 0.022999\n",
      "Train Epoch : 21 [ 57600/60000 (96%) ]\tTrain Loss : 0.175445\n",
      "\n",
      "[Epochh : 21], \tTest Loss : 0.0020, \tTest Accurary : 98.32 % \n",
      "\n",
      "Train Epoch : 22 [ 0/60000 (0%) ]\tTrain Loss : 0.120053\n",
      "Train Epoch : 22 [ 6400/60000 (11%) ]\tTrain Loss : 0.045358\n",
      "Train Epoch : 22 [ 12800/60000 (21%) ]\tTrain Loss : 0.058948\n",
      "Train Epoch : 22 [ 19200/60000 (32%) ]\tTrain Loss : 0.430635\n",
      "Train Epoch : 22 [ 25600/60000 (43%) ]\tTrain Loss : 0.051712\n",
      "Train Epoch : 22 [ 32000/60000 (53%) ]\tTrain Loss : 0.090788\n",
      "Train Epoch : 22 [ 38400/60000 (64%) ]\tTrain Loss : 0.060043\n",
      "Train Epoch : 22 [ 44800/60000 (75%) ]\tTrain Loss : 0.260993\n",
      "Train Epoch : 22 [ 51200/60000 (85%) ]\tTrain Loss : 0.072598\n",
      "Train Epoch : 22 [ 57600/60000 (96%) ]\tTrain Loss : 0.124087\n",
      "\n",
      "[Epochh : 22], \tTest Loss : 0.0020, \tTest Accurary : 98.17 % \n",
      "\n",
      "Train Epoch : 23 [ 0/60000 (0%) ]\tTrain Loss : 0.024893\n",
      "Train Epoch : 23 [ 6400/60000 (11%) ]\tTrain Loss : 0.165203\n",
      "Train Epoch : 23 [ 12800/60000 (21%) ]\tTrain Loss : 0.072246\n",
      "Train Epoch : 23 [ 19200/60000 (32%) ]\tTrain Loss : 0.020361\n",
      "Train Epoch : 23 [ 25600/60000 (43%) ]\tTrain Loss : 0.045219\n",
      "Train Epoch : 23 [ 32000/60000 (53%) ]\tTrain Loss : 0.080638\n",
      "Train Epoch : 23 [ 38400/60000 (64%) ]\tTrain Loss : 0.006979\n",
      "Train Epoch : 23 [ 44800/60000 (75%) ]\tTrain Loss : 0.052220\n",
      "Train Epoch : 23 [ 51200/60000 (85%) ]\tTrain Loss : 0.011019\n",
      "Train Epoch : 23 [ 57600/60000 (96%) ]\tTrain Loss : 0.287993\n",
      "\n",
      "[Epochh : 23], \tTest Loss : 0.0019, \tTest Accurary : 98.25 % \n",
      "\n",
      "Train Epoch : 24 [ 0/60000 (0%) ]\tTrain Loss : 0.108330\n",
      "Train Epoch : 24 [ 6400/60000 (11%) ]\tTrain Loss : 0.288167\n",
      "Train Epoch : 24 [ 12800/60000 (21%) ]\tTrain Loss : 0.015151\n",
      "Train Epoch : 24 [ 19200/60000 (32%) ]\tTrain Loss : 0.058871\n",
      "Train Epoch : 24 [ 25600/60000 (43%) ]\tTrain Loss : 0.200277\n",
      "Train Epoch : 24 [ 32000/60000 (53%) ]\tTrain Loss : 0.002855\n",
      "Train Epoch : 24 [ 38400/60000 (64%) ]\tTrain Loss : 0.118571\n",
      "Train Epoch : 24 [ 44800/60000 (75%) ]\tTrain Loss : 0.142671\n",
      "Train Epoch : 24 [ 51200/60000 (85%) ]\tTrain Loss : 0.012276\n",
      "Train Epoch : 24 [ 57600/60000 (96%) ]\tTrain Loss : 0.006980\n",
      "\n",
      "[Epochh : 24], \tTest Loss : 0.0019, \tTest Accurary : 98.29 % \n",
      "\n",
      "Train Epoch : 25 [ 0/60000 (0%) ]\tTrain Loss : 0.027431\n",
      "Train Epoch : 25 [ 6400/60000 (11%) ]\tTrain Loss : 0.285568\n",
      "Train Epoch : 25 [ 12800/60000 (21%) ]\tTrain Loss : 0.066977\n",
      "Train Epoch : 25 [ 19200/60000 (32%) ]\tTrain Loss : 0.113077\n",
      "Train Epoch : 25 [ 25600/60000 (43%) ]\tTrain Loss : 0.025024\n",
      "Train Epoch : 25 [ 32000/60000 (53%) ]\tTrain Loss : 0.140937\n",
      "Train Epoch : 25 [ 38400/60000 (64%) ]\tTrain Loss : 0.012744\n",
      "Train Epoch : 25 [ 44800/60000 (75%) ]\tTrain Loss : 0.057418\n",
      "Train Epoch : 25 [ 51200/60000 (85%) ]\tTrain Loss : 0.137790\n",
      "Train Epoch : 25 [ 57600/60000 (96%) ]\tTrain Loss : 0.014514\n",
      "\n",
      "[Epochh : 25], \tTest Loss : 0.0019, \tTest Accurary : 98.34 % \n",
      "\n",
      "Train Epoch : 26 [ 0/60000 (0%) ]\tTrain Loss : 0.158153\n",
      "Train Epoch : 26 [ 6400/60000 (11%) ]\tTrain Loss : 0.044562\n",
      "Train Epoch : 26 [ 12800/60000 (21%) ]\tTrain Loss : 0.175386\n",
      "Train Epoch : 26 [ 19200/60000 (32%) ]\tTrain Loss : 0.078177\n",
      "Train Epoch : 26 [ 25600/60000 (43%) ]\tTrain Loss : 0.075127\n",
      "Train Epoch : 26 [ 32000/60000 (53%) ]\tTrain Loss : 0.041376\n",
      "Train Epoch : 26 [ 38400/60000 (64%) ]\tTrain Loss : 0.016180\n",
      "Train Epoch : 26 [ 44800/60000 (75%) ]\tTrain Loss : 0.117381\n",
      "Train Epoch : 26 [ 51200/60000 (85%) ]\tTrain Loss : 0.017016\n",
      "Train Epoch : 26 [ 57600/60000 (96%) ]\tTrain Loss : 0.287779\n",
      "\n",
      "[Epochh : 26], \tTest Loss : 0.0018, \tTest Accurary : 98.28 % \n",
      "\n",
      "Train Epoch : 27 [ 0/60000 (0%) ]\tTrain Loss : 0.029460\n",
      "Train Epoch : 27 [ 6400/60000 (11%) ]\tTrain Loss : 0.098372\n",
      "Train Epoch : 27 [ 12800/60000 (21%) ]\tTrain Loss : 0.103828\n",
      "Train Epoch : 27 [ 19200/60000 (32%) ]\tTrain Loss : 0.030675\n",
      "Train Epoch : 27 [ 25600/60000 (43%) ]\tTrain Loss : 0.040261\n",
      "Train Epoch : 27 [ 32000/60000 (53%) ]\tTrain Loss : 0.083702\n",
      "Train Epoch : 27 [ 38400/60000 (64%) ]\tTrain Loss : 0.012508\n",
      "Train Epoch : 27 [ 44800/60000 (75%) ]\tTrain Loss : 0.141256\n",
      "Train Epoch : 27 [ 51200/60000 (85%) ]\tTrain Loss : 0.050713\n",
      "Train Epoch : 27 [ 57600/60000 (96%) ]\tTrain Loss : 0.233535\n",
      "\n",
      "[Epochh : 27], \tTest Loss : 0.0018, \tTest Accurary : 98.38 % \n",
      "\n",
      "Train Epoch : 28 [ 0/60000 (0%) ]\tTrain Loss : 0.009422\n",
      "Train Epoch : 28 [ 6400/60000 (11%) ]\tTrain Loss : 0.033264\n",
      "Train Epoch : 28 [ 12800/60000 (21%) ]\tTrain Loss : 0.208607\n",
      "Train Epoch : 28 [ 19200/60000 (32%) ]\tTrain Loss : 0.081133\n",
      "Train Epoch : 28 [ 25600/60000 (43%) ]\tTrain Loss : 0.160233\n",
      "Train Epoch : 28 [ 32000/60000 (53%) ]\tTrain Loss : 0.318216\n",
      "Train Epoch : 28 [ 38400/60000 (64%) ]\tTrain Loss : 0.060183\n",
      "Train Epoch : 28 [ 44800/60000 (75%) ]\tTrain Loss : 0.012291\n",
      "Train Epoch : 28 [ 51200/60000 (85%) ]\tTrain Loss : 0.064632\n",
      "Train Epoch : 28 [ 57600/60000 (96%) ]\tTrain Loss : 0.015851\n",
      "\n",
      "[Epochh : 28], \tTest Loss : 0.0020, \tTest Accurary : 98.22 % \n",
      "\n",
      "Train Epoch : 29 [ 0/60000 (0%) ]\tTrain Loss : 0.108141\n",
      "Train Epoch : 29 [ 6400/60000 (11%) ]\tTrain Loss : 0.039707\n",
      "Train Epoch : 29 [ 12800/60000 (21%) ]\tTrain Loss : 0.195586\n",
      "Train Epoch : 29 [ 19200/60000 (32%) ]\tTrain Loss : 0.166891\n",
      "Train Epoch : 29 [ 25600/60000 (43%) ]\tTrain Loss : 0.219184\n",
      "Train Epoch : 29 [ 32000/60000 (53%) ]\tTrain Loss : 0.218696\n",
      "Train Epoch : 29 [ 38400/60000 (64%) ]\tTrain Loss : 0.074017\n",
      "Train Epoch : 29 [ 44800/60000 (75%) ]\tTrain Loss : 0.062921\n",
      "Train Epoch : 29 [ 51200/60000 (85%) ]\tTrain Loss : 0.080091\n",
      "Train Epoch : 29 [ 57600/60000 (96%) ]\tTrain Loss : 0.044414\n",
      "\n",
      "[Epochh : 29], \tTest Loss : 0.0020, \tTest Accurary : 98.33 % \n",
      "\n",
      "Train Epoch : 30 [ 0/60000 (0%) ]\tTrain Loss : 0.111299\n",
      "Train Epoch : 30 [ 6400/60000 (11%) ]\tTrain Loss : 0.026777\n",
      "Train Epoch : 30 [ 12800/60000 (21%) ]\tTrain Loss : 0.157945\n",
      "Train Epoch : 30 [ 19200/60000 (32%) ]\tTrain Loss : 0.064704\n",
      "Train Epoch : 30 [ 25600/60000 (43%) ]\tTrain Loss : 0.033594\n",
      "Train Epoch : 30 [ 32000/60000 (53%) ]\tTrain Loss : 0.122696\n",
      "Train Epoch : 30 [ 38400/60000 (64%) ]\tTrain Loss : 0.140358\n",
      "Train Epoch : 30 [ 44800/60000 (75%) ]\tTrain Loss : 0.032416\n",
      "Train Epoch : 30 [ 51200/60000 (85%) ]\tTrain Loss : 0.010643\n",
      "Train Epoch : 30 [ 57600/60000 (96%) ]\tTrain Loss : 0.006754\n",
      "\n",
      "[Epochh : 30], \tTest Loss : 0.0019, \tTest Accurary : 98.35 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS+1) :\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[Epochh : {}], \\tTest Loss : {:.4f}, \\tTest Accurary : {:.2f} % \\n\"\n",
    "        .format(Epoch, test_loss, test_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
